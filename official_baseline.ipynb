{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b876ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# official_baseline.py (fixed)\n",
    "# Decoder-only causal LM built with PyTorch's nn.TransformerEncoder.\n",
    "\n",
    "import os, math, time, json, requests\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models import tokenizer\n",
    "\n",
    "# -----------------------------\n",
    "# Load config\n",
    "# -----------------------------\n",
    "with open(\"config/hyperparameters.json\", \"r\") as f:\n",
    "    cfg = json.load(f, object_hook=lambda d: SimpleNamespace(**d))\n",
    "\n",
    "data_path = cfg.data_path\n",
    "save_path = cfg.save_path.replace(\".pt\", \"_official.pt\")  # avoid overwriting your original\n",
    "split_ratio = tuple(cfg.split_ratio)\n",
    "block_size = cfg.block_size\n",
    "batch_size = cfg.batch_size\n",
    "patience = cfg.patience\n",
    "max_epochs = cfg.max_epochs\n",
    "eval_interval_epochs = cfg.eval_interval\n",
    "stride_overlap_ratio = cfg.stride_overlap_ratio\n",
    "\n",
    "d_model = cfg.d_model\n",
    "n_heads = cfg.n_heads\n",
    "n_layers = cfg.n_layers\n",
    "d_ff = cfg.d_ff\n",
    "dropout = cfg.dropout\n",
    "warmup = cfg.warmup\n",
    "\n",
    "grad_clip = cfg.grad_clip\n",
    "seed = cfg.seed\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47e2316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data/full_shakespeare.txt' already exists, skipping download.\n",
      "100 unique chars\n",
      "vocab_size = 100\n",
      "Total tokens: 5,359,388\n",
      "Train: 4,287,510, Val: 535,938, Test: 535,940\n",
      "Train batches: 1047, Val batches: 131\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Dataset fetch\n",
    "# -----------------------------\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "if data_path == \"data/tiny_shakespeare.txt\":\n",
    "    if os.path.exists(data_path):\n",
    "        print(f\"'{data_path}' already exists, skipping download.\")\n",
    "    else:\n",
    "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "        text = requests.get(url).text\n",
    "        with open(\"data/tiny_shakespeare.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(\"Tiny Shakespeare downloaded! File size:\", len(text), \"characters\")\n",
    "elif data_path == \"data/full_shakespeare.txt\":\n",
    "    if os.path.exists(data_path):\n",
    "        print(f\"'{data_path}' already exists, skipping download.\")\n",
    "    else:\n",
    "        url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "        print(\"Downloading full Shakespeare from Project Gutenberg...\")\n",
    "        text = requests.get(url).text\n",
    "        if \"*** START\" in text:\n",
    "            text = text.split(\"*** START\")[1]\n",
    "        if \"*** END\" in text:\n",
    "            text = text.split(\"*** END\")[0]\n",
    "        with open(data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(\"Full Shakespeare downloaded! File size:\", len(text), \"characters\")\n",
    "else:\n",
    "    raise SystemExit(\"Unexpected dataset, stop training.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer & splits\n",
    "# -----------------------------\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "tok = tokenizer.CharTokenizer(text)\n",
    "print(len(tok.chars), \"unique chars\")\n",
    "\n",
    "ids = tok.encode(text)\n",
    "data = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "vocab_size = getattr(tok, \"vocab_size\", len(tok.chars))\n",
    "print(\"vocab_size =\", vocab_size)\n",
    "mx = int(max(ids)) if len(ids) > 0 else -1\n",
    "assert mx < int(vocab_size), f\"max id {mx} >= vocab_size {int(vocab_size)}\"\n",
    "\n",
    "n = len(data)\n",
    "n_train = int(split_ratio[0] * n)\n",
    "n_val = int(split_ratio[1] * n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "train_data = data[:n_train]\n",
    "val_data = data[n_train:n_train + n_val]\n",
    "test_data = data[n_train + n_val:]\n",
    "\n",
    "print(f\"Total tokens: {n:,}\")\n",
    "print(f\"Train: {len(train_data):,}, Val: {len(val_data):,}, Test: {len(test_data):,}\")\n",
    "\n",
    "class CharDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, block_size, stride=None):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "        self.stride = self.block_size if (stride is None or stride < 1) else stride\n",
    "        last_valid_start_idx = len(self.data) - self.block_size - 1\n",
    "        self.num_samples = 0 if last_valid_start_idx < 0 else (last_valid_start_idx // self.stride) + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = idx * self.stride\n",
    "        x = self.data[actual_idx : actual_idx + self.block_size]\n",
    "        y = self.data[actual_idx + 1 : actual_idx + 1 + self.block_size]\n",
    "        return x, y\n",
    "\n",
    "my_stride = int(block_size * stride_overlap_ratio)\n",
    "train_dataset = CharDataset(train_data, block_size, stride=my_stride)\n",
    "val_dataset   = CharDataset(val_data,   block_size, stride=my_stride)\n",
    "test_dataset  = CharDataset(test_data,  block_size, stride=my_stride)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  pin_memory=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dde9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Official-ish Decoder-only Transformer using nn.TransformerEncoder (causal mask)\n",
    "# -----------------------------\n",
    "class OfficialishCharLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, dropout, block_size):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.tok = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = nn.Embedding(block_size, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n",
    "            dropout=dropout, batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        # Weight tying\n",
    "        self.lm_head.weight = self.tok.weight\n",
    "\n",
    "    def _causal_mask(self, T, device):\n",
    "        # float mask with -inf on future positions\n",
    "        return torch.triu(torch.ones(T, T, device=device) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        if T > self.block_size:\n",
    "            raise ValueError(f\"Sequence length {T} exceeds block_size {self.block_size}\")\n",
    "\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)  # [1, T]\n",
    "        x = self.tok(idx) * math.sqrt(self.d_model) + self.pos(pos)  # [B, T, C]\n",
    "\n",
    "        causal = self._causal_mask(T, idx.device)  # [T, T]\n",
    "        y = self.encoder(x, mask=causal)  # [B, T, C], encoder-only with causal mask\n",
    "        y = self.ln_f(y)\n",
    "        logits = self.lm_head(y)  # [B, T, V]\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens=200, temperature=1.0, top_k=None):\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, k=top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "        return idx\n",
    "\n",
    "# -----------------------------\n",
    "# Noam scheduler wrapper\n",
    "# -----------------------------\n",
    "class NoamOpt:\n",
    "    def __init__(self, d_model, warmup, optimizer):\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = d_model ** (-0.5)\n",
    "        self.opt = optimizer\n",
    "        self._lr = 0.0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        lr = self.factor * min(self._step ** (-0.5), self._step * (self.warmup ** -1.5))\n",
    "        for g in self.opt.param_groups:\n",
    "            g['lr'] = lr\n",
    "        self._lr = lr\n",
    "        self.opt.step()\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81738aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Project_GenAI\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "d:\\Anaconda\\envs\\Project_GenAI\\Lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "d:\\Anaconda\\envs\\Project_GenAI\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe val (lower is better): {500: 4.38028018951416, 1000: 6.238727722167969, 2000: 25.882893676757813}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{500: 4.38028018951416, 1000: 6.238727722167969, 2000: 25.882893676757813}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quick_lr_probe(warmups=(500, 1000, 2000)):\n",
    "    probe = {}\n",
    "    for wu in warmups:\n",
    "        torch.manual_seed(seed)\n",
    "        m = OfficialishCharLM(vocab_size, d_model, n_heads, n_layers, d_ff, dropout, block_size).to(device)\n",
    "        base = torch.optim.Adam(m.parameters(), betas=(0.9,0.98), eps=1e-9)\n",
    "        sched = NoamOpt(d_model, warmup=wu, optimizer=base)\n",
    "        it = iter(train_loader)\n",
    "        losses = []\n",
    "        for _ in range(200):\n",
    "            try:\n",
    "                xb, yb = next(it)\n",
    "            except StopIteration:\n",
    "                it = iter(train_loader); xb, yb = next(it)\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            _, loss = m(xb, yb)\n",
    "            base.zero_grad(); loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(m.parameters(), grad_clip)\n",
    "            sched.step()\n",
    "            losses.append(loss.item())\n",
    "        probe[wu] = sum(losses[-50:]) / 50 \n",
    "    print(\"Probe val (lower is better):\", probe)\n",
    "    return probe\n",
    "\n",
    "quick_lr_probe(warmups=(500, 1000, 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc06a745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe val (lower is better): {100: 3.479309668540955, 200: 3.6915267372131346, 300: 3.8168939924240113}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{100: 3.479309668540955, 200: 3.6915267372131346, 300: 3.8168939924240113}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_lr_probe(warmups=(100, 200, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0b82ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe val (lower is better): {10: 3.3596645832061767, 30: 3.370869073867798, 50: 4.400758528709412}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{10: 3.3596645832061767, 30: 3.370869073867798, 50: 4.400758528709412}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_lr_probe(warmups=(10, 30, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9513f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Model / Optimizer / Logger\n",
    "# -----------------------------\n",
    "torch.manual_seed(seed)\n",
    "model = OfficialishCharLM(vocab_size, d_model, n_heads, n_layers, d_ff, dropout, block_size).to(device)\n",
    "print(\"Params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "run_dir = f\"runs/official_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=run_dir)\n",
    "print(\"TensorBoard logdir:\", run_dir)\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "base_opt = torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0)\n",
    "opt = NoamOpt(d_model, warmup=warmup, optimizer=base_opt)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "bad_epochs = 0\n",
    "global_step = 0\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        _, loss = model(xb, yb)\n",
    "        total_loss += loss.item()\n",
    "    model.train()\n",
    "    return total_loss / max(len(loader), 1)\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    model.train()\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # Gradient accumulation (set >1 if want to simulate larger batches)\n",
    "    accum_steps = 1\n",
    "\n",
    "    # tqdm progress bar for the current epoch\n",
    "    pbar = tqdm(\n",
    "        enumerate(train_loader),\n",
    "        total=len(train_loader),\n",
    "        desc=f\"Epoch {epoch}/{max_epochs}\",\n",
    "        dynamic_ncols=True,\n",
    "        leave=False,  # keep only one dynamic line, no screen flooding\n",
    "    )\n",
    "\n",
    "    for batch_idx, (xb, yb) in pbar:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        # Forward + loss\n",
    "        _, loss = model(xb, yb)\n",
    "        loss = loss / accum_steps\n",
    "\n",
    "        # Backward + gradient clipping + optimizer step\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = float(torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip))\n",
    "        if (batch_idx + 1) % accum_steps == 0:\n",
    "            opt.step()\n",
    "\n",
    "        # Detach scalar values for display/logging\n",
    "        loss_item = loss.item() * accum_steps\n",
    "        bpc = loss_item / math.log(2)\n",
    "        ppl = math.exp(loss_item) if loss_item < 20 else float(\"inf\")\n",
    "\n",
    "        # Update tqdm line with key metrics\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{loss_item:.4f}\",\n",
    "            \"bpc\": f\"{bpc:.3f}\",\n",
    "            \"ppl\": f\"{ppl:.1f}\" if ppl != float(\"inf\") else \"inf\",\n",
    "            \"lr\": f\"{opt.lr:.6f}\",\n",
    "            \"gnorm\": f\"{grad_norm:.2f}\",\n",
    "        })\n",
    "\n",
    "        # TensorBoard logging\n",
    "        writer.add_scalar(\"loss/train_batch\", loss_item, global_step)\n",
    "        writer.add_scalar(\"lr\", opt.lr, global_step)\n",
    "        writer.add_scalar(\"grad_norm\", grad_norm, global_step)\n",
    "        global_step += 1\n",
    "\n",
    "    # End of epoch summary\n",
    "    # tqdm.write(f\"[Epoch {epoch}] time={time.time() - epoch_start_time:.1f}s\")\n",
    "\n",
    "    # ---- Validation phase ----\n",
    "    val_loss = evaluate(val_loader)\n",
    "    bpc = val_loss / math.log(2)\n",
    "    tqdm.write(f\"[Epoch {epoch}] Val Loss {val_loss:.4f} | Val BPC {bpc:.4f} | time={time.time() - epoch_start_time:.1f}s\")\n",
    "\n",
    "    writer.add_scalar(\"loss/val\", val_loss, epoch)\n",
    "    writer.add_scalar(\"metrics/val_bpc\", bpc, epoch)\n",
    "\n",
    "    # ---- Early stopping and checkpointing ----\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        bad_epochs = 0\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": base_opt.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_val\": best_val,\n",
    "            \"tok_chars\": tok.chars,\n",
    "            \"config\": dict(d_model=d_model, n_heads=n_heads, n_layers=n_layers, d_ff=d_ff,\n",
    "                           dropout=dropout, block_size=block_size, vocab_size=vocab_size)\n",
    "        }, save_path)\n",
    "        tqdm.write(f\"Improved! Best val_loss={best_val:.4f} (saved to {save_path})\")\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        tqdm.write(f\"No improvement ({bad_epochs}/{patience})\")\n",
    "        if bad_epochs >= patience:\n",
    "            tqdm.write(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe02fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Test best checkpoint + a sample generation\n",
    "# -----------------------------\n",
    "checkpoint = torch.load(save_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model with best_val={checkpoint['best_val']:.4f} at epoch={checkpoint['epoch']}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_simple(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        _, loss = model(xb, yb)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(len(loader), 1)\n",
    "\n",
    "test_loss = evaluate_simple(test_loader)\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Perplexity (PPL): {torch.exp(torch.tensor(test_loss)):.2f}\")\n",
    "print(f\"BPC: {(test_loss / math.log(2)):.4f}\")\n",
    "\n",
    "# Sample generation from a prompt\n",
    "prompt = \"ROMEO:\"\n",
    "start_ids = tok.encode(prompt)\n",
    "idx = torch.tensor([start_ids], dtype=torch.long, device=device)\n",
    "out = model.generate(idx, max_new_tokens=400, temperature=1.0, top_k=None)\n",
    "print(tok.decode(out[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb1b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### GRID SEARCH #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f2aa485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math, torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Run 1or3 Epoch for grid search\n",
    "def run_experiment(d_model, n_heads, n_layers, d_ff, dropout, block_size, warmup,\n",
    "                   batch_size=32, max_epochs=1): \n",
    "    torch.cuda.empty_cache()\n",
    "    torch.manual_seed(42)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # ---- DataLoader rebuild ----\n",
    "    train_dataset = CharDataset(train_data, block_size)\n",
    "    val_dataset   = CharDataset(val_data, block_size)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # ---- Model ----\n",
    "    model = OfficialishCharLM(vocab_size, d_model, n_heads, n_layers, d_ff, dropout, block_size).to(device)\n",
    "    base_opt = torch.optim.Adam(model.parameters(), betas=(0.9,0.98), eps=1e-9)\n",
    "    opt = NoamOpt(d_model, warmup, base_opt)\n",
    "\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    for xb, yb in tqdm(train_loader, total=len(train_loader),\n",
    "                       desc=f\"d_model={d_model}, n_layers={n_layers}, dropout={dropout}, block={block_size}\"):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        _, loss = model(xb, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        opt.step()\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    # ---- Validation ----\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            _, loss = model(xb, yb)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    bpc = val_loss / math.log(2)\n",
    "\n",
    "    return dict(\n",
    "        d_model=d_model, n_layers=n_layers, block_size=block_size,\n",
    "        dropout=dropout, warmup=warmup, batch_size=batch_size,\n",
    "        val_loss=val_loss, bpc=bpc, train_time=train_time\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4757220a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configs: 108\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "d_models = [128, 256, 384, 512]\n",
    "n_layers = [4, 6, 8]\n",
    "dropouts = [0.0, 0.1, 0.2]\n",
    "block_sizes = [128, 256, 512]\n",
    "warmup = 10\n",
    "\n",
    "search_space = []\n",
    "for d_model, n_layer, drop, block in itertools.product(d_models, n_layers, dropouts, block_sizes):\n",
    "    n_heads = max(2, d_model // 64)  # keep roughly 64-dim per head\n",
    "    d_ff = 4 * d_model\n",
    "    search_space.append(dict(\n",
    "        d_model=d_model, n_heads=n_heads, n_layers=n_layer,\n",
    "        d_ff=d_ff, dropout=drop, block_size=block, warmup=warmup\n",
    "    ))\n",
    "\n",
    "print(f\"Total configs: {len(search_space)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8073b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_TOKENS = 65536  # per optimizer step\n",
    "def plan_bs(block):\n",
    "    return max(1, TARGET_TOKENS // block)\n",
    "\n",
    "for cfg in search_space:\n",
    "    cfg[\"batch_size\"] = plan_bs(cfg[\"block_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a31c3f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d_model=128, n_layers=4, dropout=0.0, block=128: 100%|██████████| 66/66 [00:10<00:00,  6.05it/s]\n",
      "d_model=128, n_layers=4, dropout=0.0, block=256: 100%|██████████| 66/66 [00:11<00:00,  5.89it/s]\n",
      "d_model=128, n_layers=4, dropout=0.0, block=512: 100%|██████████| 66/66 [00:12<00:00,  5.17it/s]\n",
      "d_model=128, n_layers=4, dropout=0.1, block=128: 100%|██████████| 66/66 [00:11<00:00,  5.53it/s]\n",
      "d_model=128, n_layers=4, dropout=0.1, block=256: 100%|██████████| 66/66 [00:12<00:00,  5.32it/s]\n",
      "d_model=128, n_layers=4, dropout=0.1, block=512: 100%|██████████| 66/66 [00:13<00:00,  4.84it/s]\n",
      "d_model=128, n_layers=4, dropout=0.2, block=128: 100%|██████████| 66/66 [00:11<00:00,  5.64it/s]\n",
      "d_model=128, n_layers=4, dropout=0.2, block=256: 100%|██████████| 66/66 [00:12<00:00,  5.33it/s]\n",
      "d_model=128, n_layers=4, dropout=0.2, block=512: 100%|██████████| 66/66 [00:13<00:00,  4.88it/s]\n",
      "d_model=128, n_layers=6, dropout=0.0, block=128: 100%|██████████| 66/66 [00:15<00:00,  4.20it/s]\n",
      "d_model=128, n_layers=6, dropout=0.0, block=256: 100%|██████████| 66/66 [00:16<00:00,  4.08it/s]\n",
      "d_model=128, n_layers=6, dropout=0.0, block=512: 100%|██████████| 66/66 [00:18<00:00,  3.65it/s]\n",
      "d_model=128, n_layers=6, dropout=0.1, block=128: 100%|██████████| 66/66 [00:17<00:00,  3.88it/s]\n",
      "d_model=128, n_layers=6, dropout=0.1, block=256: 100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n",
      "d_model=128, n_layers=6, dropout=0.1, block=512: 100%|██████████| 66/66 [00:20<00:00,  3.29it/s]\n",
      "d_model=128, n_layers=6, dropout=0.2, block=128: 100%|██████████| 66/66 [00:16<00:00,  3.90it/s]\n",
      "d_model=128, n_layers=6, dropout=0.2, block=256: 100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n",
      "d_model=128, n_layers=6, dropout=0.2, block=512: 100%|██████████| 66/66 [00:19<00:00,  3.32it/s]\n",
      "d_model=128, n_layers=8, dropout=0.0, block=128: 100%|██████████| 66/66 [00:20<00:00,  3.26it/s]\n",
      "d_model=128, n_layers=8, dropout=0.0, block=256: 100%|██████████| 66/66 [00:21<00:00,  3.06it/s]\n",
      "d_model=128, n_layers=8, dropout=0.0, block=512: 100%|██████████| 66/66 [00:23<00:00,  2.77it/s]\n",
      "d_model=128, n_layers=8, dropout=0.1, block=128: 100%|██████████| 66/66 [00:22<00:00,  2.94it/s]\n",
      "d_model=128, n_layers=8, dropout=0.1, block=256: 100%|██████████| 66/66 [00:23<00:00,  2.78it/s]\n",
      "d_model=128, n_layers=8, dropout=0.1, block=512: 100%|██████████| 66/66 [00:26<00:00,  2.49it/s]\n",
      "d_model=128, n_layers=8, dropout=0.2, block=128: 100%|██████████| 66/66 [00:22<00:00,  2.89it/s]\n",
      "d_model=128, n_layers=8, dropout=0.2, block=256: 100%|██████████| 66/66 [00:23<00:00,  2.76it/s]\n",
      "d_model=128, n_layers=8, dropout=0.2, block=512: 100%|██████████| 66/66 [00:26<00:00,  2.51it/s]\n",
      "d_model=256, n_layers=4, dropout=0.0, block=128: 100%|██████████| 66/66 [00:25<00:00,  2.63it/s]\n",
      "d_model=256, n_layers=4, dropout=0.0, block=256: 100%|██████████| 66/66 [00:26<00:00,  2.52it/s]\n",
      "d_model=256, n_layers=4, dropout=0.0, block=512: 100%|██████████| 66/66 [00:29<00:00,  2.26it/s]\n",
      "d_model=256, n_layers=4, dropout=0.1, block=128: 100%|██████████| 66/66 [00:27<00:00,  2.41it/s]\n",
      "d_model=256, n_layers=4, dropout=0.1, block=256: 100%|██████████| 66/66 [00:28<00:00,  2.34it/s]\n",
      "d_model=256, n_layers=4, dropout=0.1, block=512: 100%|██████████| 66/66 [00:31<00:00,  2.11it/s]\n",
      "d_model=256, n_layers=4, dropout=0.2, block=128: 100%|██████████| 66/66 [00:26<00:00,  2.44it/s]\n",
      "d_model=256, n_layers=4, dropout=0.2, block=256: 100%|██████████| 66/66 [00:28<00:00,  2.33it/s]\n",
      "d_model=256, n_layers=4, dropout=0.2, block=512: 100%|██████████| 66/66 [00:31<00:00,  2.11it/s]\n",
      "d_model=256, n_layers=6, dropout=0.0, block=128: 100%|██████████| 66/66 [00:36<00:00,  1.81it/s]\n",
      "d_model=256, n_layers=6, dropout=0.0, block=256: 100%|██████████| 66/66 [00:38<00:00,  1.70it/s]\n",
      "d_model=256, n_layers=6, dropout=0.0, block=512: 100%|██████████| 66/66 [00:42<00:00,  1.54it/s]\n",
      "d_model=256, n_layers=6, dropout=0.1, block=128: 100%|██████████| 66/66 [01:34<00:00,  1.44s/it]\n",
      "d_model=256, n_layers=6, dropout=0.1, block=256: 100%|██████████| 66/66 [01:35<00:00,  1.44s/it]\n",
      "d_model=256, n_layers=6, dropout=0.1, block=512: 100%|██████████| 66/66 [02:02<00:00,  1.86s/it]\n",
      "d_model=256, n_layers=6, dropout=0.2, block=128: 100%|██████████| 66/66 [01:25<00:00,  1.29s/it]\n",
      "d_model=256, n_layers=6, dropout=0.2, block=256: 100%|██████████| 66/66 [01:30<00:00,  1.37s/it]\n",
      "d_model=256, n_layers=6, dropout=0.2, block=512: 100%|██████████| 66/66 [01:59<00:00,  1.82s/it]\n",
      "d_model=256, n_layers=8, dropout=0.0, block=128: 100%|██████████| 66/66 [00:54<00:00,  1.20it/s]\n",
      "d_model=256, n_layers=8, dropout=0.0, block=256: 100%|██████████| 66/66 [00:58<00:00,  1.12it/s]\n",
      "d_model=256, n_layers=8, dropout=0.0, block=512: 100%|██████████| 66/66 [02:06<00:00,  1.92s/it]\n",
      "d_model=256, n_layers=8, dropout=0.1, block=128: 100%|██████████| 66/66 [04:26<00:00,  4.04s/it]\n",
      "d_model=256, n_layers=8, dropout=0.1, block=256: 100%|██████████| 66/66 [05:09<00:00,  4.69s/it]\n",
      "d_model=256, n_layers=8, dropout=0.1, block=512: 100%|██████████| 66/66 [05:25<00:00,  4.93s/it]\n",
      "d_model=256, n_layers=8, dropout=0.2, block=128: 100%|██████████| 66/66 [05:27<00:00,  4.95s/it]\n",
      "d_model=256, n_layers=8, dropout=0.2, block=256: 100%|██████████| 66/66 [04:49<00:00,  4.39s/it]\n",
      "d_model=256, n_layers=8, dropout=0.2, block=512: 100%|██████████| 66/66 [05:12<00:00,  4.73s/it]\n",
      "d_model=384, n_layers=4, dropout=0.0, block=128: 100%|██████████| 66/66 [00:44<00:00,  1.49it/s]\n",
      "d_model=384, n_layers=4, dropout=0.0, block=256: 100%|██████████| 66/66 [00:47<00:00,  1.40it/s]\n",
      "d_model=384, n_layers=4, dropout=0.0, block=512: 100%|██████████| 66/66 [00:51<00:00,  1.29it/s]\n",
      "d_model=384, n_layers=4, dropout=0.1, block=128: 100%|██████████| 66/66 [01:28<00:00,  1.35s/it]\n",
      "d_model=384, n_layers=4, dropout=0.1, block=256: 100%|██████████| 66/66 [01:32<00:00,  1.40s/it]\n",
      "d_model=384, n_layers=4, dropout=0.1, block=512: 100%|██████████| 66/66 [01:36<00:00,  1.46s/it]\n",
      "d_model=384, n_layers=4, dropout=0.2, block=128: 100%|██████████| 66/66 [01:29<00:00,  1.36s/it]\n",
      "d_model=384, n_layers=4, dropout=0.2, block=256: 100%|██████████| 66/66 [01:30<00:00,  1.37s/it]\n",
      "d_model=384, n_layers=4, dropout=0.2, block=512: 100%|██████████| 66/66 [01:34<00:00,  1.44s/it]\n",
      "d_model=384, n_layers=6, dropout=0.0, block=128:  23%|██▎       | 15/66 [00:37<02:07,  2.50s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cfg \u001b[38;5;129;01min\u001b[39;00m search_space:\n\u001b[1;32m----> 3\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[1;32mIn[27], line 30\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(d_model, n_heads, n_layers, d_ff, dropout, block_size, warmup, batch_size, max_epochs)\u001b[0m\n\u001b[0;32m     28\u001b[0m _, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[0;32m     29\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), grad_clip)\n\u001b[0;32m     32\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Project_GenAI\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Project_GenAI\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for cfg in search_space:\n",
    "    result = run_experiment(**cfg, max_epochs=3)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9d4da28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d_model</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>block_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>warmup</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>bpc</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>3.196448</td>\n",
       "      <td>4.611500</td>\n",
       "      <td>12.401019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>3.212545</td>\n",
       "      <td>4.634723</td>\n",
       "      <td>11.949633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>3.215670</td>\n",
       "      <td>4.639231</td>\n",
       "      <td>13.633289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>256</td>\n",
       "      <td>8</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>3.218518</td>\n",
       "      <td>4.643340</td>\n",
       "      <td>327.032681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>3.221088</td>\n",
       "      <td>4.647048</td>\n",
       "      <td>10.911317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>256</td>\n",
       "      <td>6</td>\n",
       "      <td>512</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>3.457651</td>\n",
       "      <td>4.988335</td>\n",
       "      <td>120.000451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>384</td>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>3.582868</td>\n",
       "      <td>5.168986</td>\n",
       "      <td>92.691968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>384</td>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>3.623940</td>\n",
       "      <td>5.228241</td>\n",
       "      <td>47.270605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>384</td>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>3.685264</td>\n",
       "      <td>5.316712</td>\n",
       "      <td>94.766493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>384</td>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>3.910361</td>\n",
       "      <td>5.641458</td>\n",
       "      <td>90.452285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    d_model  n_layers  block_size  dropout  warmup  batch_size  val_loss  \\\n",
       "4       128         4         256      0.1      10         256  3.196448   \n",
       "3       128         4         128      0.1      10         512  3.212545   \n",
       "5       128         4         512      0.1      10         128  3.215670   \n",
       "51      256         8         128      0.2      10         512  3.218518   \n",
       "0       128         4         128      0.0      10         512  3.221088   \n",
       "..      ...       ...         ...      ...     ...         ...       ...   \n",
       "44      256         6         512      0.2      10         128  3.457651   \n",
       "58      384         4         256      0.1      10         256  3.582868   \n",
       "55      384         4         256      0.0      10         256  3.623940   \n",
       "62      384         4         512      0.2      10         128  3.685264   \n",
       "61      384         4         256      0.2      10         256  3.910361   \n",
       "\n",
       "         bpc  train_time  \n",
       "4   4.611500   12.401019  \n",
       "3   4.634723   11.949633  \n",
       "5   4.639231   13.633289  \n",
       "51  4.643340  327.032681  \n",
       "0   4.647048   10.911317  \n",
       "..       ...         ...  \n",
       "44  4.988335  120.000451  \n",
       "58  5.168986   92.691968  \n",
       "55  5.228241   47.270605  \n",
       "62  5.316712   94.766493  \n",
       "61  5.641458   90.452285  \n",
       "\n",
       "[63 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df = df.sort_values(\"bpc\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
