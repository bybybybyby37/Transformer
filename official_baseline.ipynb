{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b876ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# official_baseline.py (fixed)\n",
    "# Decoder-only causal LM built with PyTorch's nn.TransformerEncoder.\n",
    "\n",
    "import os, math, time, json, requests\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models import tokenizer\n",
    "\n",
    "# -----------------------------\n",
    "# Load config\n",
    "# -----------------------------\n",
    "with open(\"config\\\\hyperparameters.json\", \"r\") as f:\n",
    "    cfg = json.load(f, object_hook=lambda d: SimpleNamespace(**d))\n",
    "\n",
    "data_path = cfg.data_path\n",
    "save_path = cfg.save_path.replace(\".pt\", \"_official.pt\")  # avoid overwriting your original\n",
    "split_ratio = tuple(cfg.split_ratio)\n",
    "block_size = cfg.block_size\n",
    "batch_size = cfg.batch_size\n",
    "patience = cfg.patience\n",
    "max_epochs = cfg.max_epochs\n",
    "eval_interval_epochs = cfg.eval_interval\n",
    "stride_overlap_ratio = cfg.stride_overlap_ratio\n",
    "\n",
    "d_model = cfg.d_model\n",
    "n_heads = cfg.n_heads\n",
    "n_layers = cfg.n_layers\n",
    "d_ff = cfg.d_ff\n",
    "dropout = cfg.dropout\n",
    "warmup = cfg.warmup\n",
    "\n",
    "grad_clip = cfg.grad_clip\n",
    "seed = cfg.seed\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47e2316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data/full_shakespeare.txt' already exists, skipping download.\n",
      "100 unique chars\n",
      "vocab_size = 100\n",
      "Total tokens: 5,359,388\n",
      "Train: 4,287,510, Val: 535,938, Test: 535,940\n",
      "Train batches: 1047, Val batches: 131\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Dataset fetch\n",
    "# -----------------------------\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "if data_path == \"data/tiny_shakespeare.txt\":\n",
    "    if os.path.exists(data_path):\n",
    "        print(f\"'{data_path}' already exists, skipping download.\")\n",
    "    else:\n",
    "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "        text = requests.get(url).text\n",
    "        with open(\"data/tiny_shakespeare.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(\"Tiny Shakespeare downloaded! File size:\", len(text), \"characters\")\n",
    "elif data_path == \"data/full_shakespeare.txt\":\n",
    "    if os.path.exists(data_path):\n",
    "        print(f\"'{data_path}' already exists, skipping download.\")\n",
    "    else:\n",
    "        url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "        print(\"Downloading full Shakespeare from Project Gutenberg...\")\n",
    "        text = requests.get(url).text\n",
    "        if \"*** START\" in text:\n",
    "            text = text.split(\"*** START\")[1]\n",
    "        if \"*** END\" in text:\n",
    "            text = text.split(\"*** END\")[0]\n",
    "        with open(data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(\"Full Shakespeare downloaded! File size:\", len(text), \"characters\")\n",
    "else:\n",
    "    raise SystemExit(\"Unexpected dataset, stop training.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer & splits\n",
    "# -----------------------------\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "tok = tokenizer.CharTokenizer(text)\n",
    "print(len(tok.chars), \"unique chars\")\n",
    "\n",
    "ids = tok.encode(text)\n",
    "data = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "vocab_size = getattr(tok, \"vocab_size\", len(tok.chars))\n",
    "print(\"vocab_size =\", vocab_size)\n",
    "mx = int(max(ids)) if len(ids) > 0 else -1\n",
    "assert mx < int(vocab_size), f\"max id {mx} >= vocab_size {int(vocab_size)}\"\n",
    "\n",
    "n = len(data)\n",
    "n_train = int(split_ratio[0] * n)\n",
    "n_val = int(split_ratio[1] * n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "train_data = data[:n_train]\n",
    "val_data = data[n_train:n_train + n_val]\n",
    "test_data = data[n_train + n_val:]\n",
    "\n",
    "print(f\"Total tokens: {n:,}\")\n",
    "print(f\"Train: {len(train_data):,}, Val: {len(val_data):,}, Test: {len(test_data):,}\")\n",
    "\n",
    "class CharDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, block_size, stride=None):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "        self.stride = self.block_size if (stride is None or stride < 1) else stride\n",
    "        last_valid_start_idx = len(self.data) - self.block_size - 1\n",
    "        self.num_samples = 0 if last_valid_start_idx < 0 else (last_valid_start_idx // self.stride) + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = idx * self.stride\n",
    "        x = self.data[actual_idx : actual_idx + self.block_size]\n",
    "        y = self.data[actual_idx + 1 : actual_idx + 1 + self.block_size]\n",
    "        return x, y\n",
    "\n",
    "my_stride = int(block_size * stride_overlap_ratio)\n",
    "train_dataset = CharDataset(train_data, block_size, stride=my_stride)\n",
    "val_dataset   = CharDataset(val_data,   block_size, stride=my_stride)\n",
    "test_dataset  = CharDataset(test_data,  block_size, stride=my_stride)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  pin_memory=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dde9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Official-ish Decoder-only Transformer using nn.TransformerEncoder (causal mask)\n",
    "# -----------------------------\n",
    "class OfficialishCharLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, dropout, block_size):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.tok = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = nn.Embedding(block_size, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n",
    "            dropout=dropout, batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        # Weight tying\n",
    "        self.lm_head.weight = self.tok.weight\n",
    "\n",
    "    def _causal_mask(self, T, device):\n",
    "        # float mask with -inf on future positions\n",
    "        return torch.triu(torch.ones(T, T, device=device) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        if T > self.block_size:\n",
    "            raise ValueError(f\"Sequence length {T} exceeds block_size {self.block_size}\")\n",
    "\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)  # [1, T]\n",
    "        x = self.tok(idx) * math.sqrt(self.d_model) + self.pos(pos)  # [B, T, C]\n",
    "\n",
    "        causal = self._causal_mask(T, idx.device)  # [T, T]\n",
    "        y = self.encoder(x, mask=causal)  # [B, T, C], encoder-only with causal mask\n",
    "        y = self.ln_f(y)\n",
    "        logits = self.lm_head(y)  # [B, T, V]\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens=200, temperature=1.0, top_k=None):\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, k=top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "        return idx\n",
    "\n",
    "# -----------------------------\n",
    "# Noam scheduler wrapper\n",
    "# -----------------------------\n",
    "class NoamOpt:\n",
    "    def __init__(self, d_model, warmup, optimizer):\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = d_model ** (-0.5)\n",
    "        self.opt = optimizer\n",
    "        self._lr = 0.0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        lr = self.factor * min(self._step ** (-0.5), self._step * (self.warmup ** -1.5))\n",
    "        for g in self.opt.param_groups:\n",
    "            g['lr'] = lr\n",
    "        self._lr = lr\n",
    "        self.opt.step()\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81738aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Project_GenAI\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "d:\\Anaconda\\envs\\Project_GenAI\\Lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "d:\\Anaconda\\envs\\Project_GenAI\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe val (lower is better): {500: 4.380279684066773, 1000: 6.238727807998657, 2000: 25.882894420623778}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{500: 4.380279684066773, 1000: 6.238727807998657, 2000: 25.882894420623778}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quick_lr_probe(warmups=(500, 1000, 2000)):\n",
    "    probe = {}\n",
    "    for wu in warmups:\n",
    "        torch.manual_seed(seed)\n",
    "        m = OfficialishCharLM(vocab_size, d_model, n_heads, n_layers, d_ff, dropout, block_size).to(device)\n",
    "        base = torch.optim.Adam(m.parameters(), betas=(0.9,0.98), eps=1e-9)\n",
    "        sched = NoamOpt(d_model, warmup=wu, optimizer=base)\n",
    "        it = iter(train_loader)\n",
    "        losses = []\n",
    "        for _ in range(200):\n",
    "            try:\n",
    "                xb, yb = next(it)\n",
    "            except StopIteration:\n",
    "                it = iter(train_loader); xb, yb = next(it)\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            _, loss = m(xb, yb)\n",
    "            base.zero_grad(); loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(m.parameters(), grad_clip)\n",
    "            sched.step()\n",
    "            losses.append(loss.item())\n",
    "        probe[wu] = sum(losses[-50:]) / 50 \n",
    "    print(\"Probe val (lower is better):\", probe)\n",
    "    return probe\n",
    "\n",
    "quick_lr_probe(warmups=(500, 1000, 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc06a745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe val (lower is better): {300: 4.0604815006256105, 500: 4.380278787612915, 700: 4.92201642036438}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{300: 4.0604815006256105, 500: 4.380278787612915, 700: 4.92201642036438}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_lr_probe(warmups=(300, 500, 700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a0b82ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe val (lower is better): {10: 3.374137239456177, 30: 3.5210319900512697, 50: 3.4410608959198}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{10: 3.374137239456177, 30: 3.5210319900512697, 50: 3.4410608959198}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_lr_probe(warmups=(10, 30, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9513f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Model / Optimizer / Logger\n",
    "# -----------------------------\n",
    "torch.manual_seed(seed)\n",
    "model = OfficialishCharLM(vocab_size, d_model, n_heads, n_layers, d_ff, dropout, block_size).to(device)\n",
    "print(\"Params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "run_dir = f\"runs/official_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=run_dir)\n",
    "print(\"TensorBoard logdir:\", run_dir)\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "base_opt = torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0)\n",
    "opt = NoamOpt(d_model, warmup=warmup, optimizer=base_opt)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "bad_epochs = 0\n",
    "global_step = 0\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        _, loss = model(xb, yb)\n",
    "        total_loss += loss.item()\n",
    "    model.train()\n",
    "    return total_loss / max(len(loader), 1)\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    model.train()\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # Gradient accumulation (set >1 if want to simulate larger batches)\n",
    "    accum_steps = 1\n",
    "\n",
    "    # tqdm progress bar for the current epoch\n",
    "    pbar = tqdm(\n",
    "        enumerate(train_loader),\n",
    "        total=len(train_loader),\n",
    "        desc=f\"Epoch {epoch}/{max_epochs}\",\n",
    "        dynamic_ncols=True,\n",
    "        leave=False,  # keep only one dynamic line, no screen flooding\n",
    "    )\n",
    "\n",
    "    for batch_idx, (xb, yb) in pbar:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        # Forward + loss\n",
    "        _, loss = model(xb, yb)\n",
    "        loss = loss / accum_steps\n",
    "\n",
    "        # Backward + gradient clipping + optimizer step\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = float(torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip))\n",
    "        if (batch_idx + 1) % accum_steps == 0:\n",
    "            opt.step()\n",
    "\n",
    "        # Detach scalar values for display/logging\n",
    "        loss_item = loss.item() * accum_steps\n",
    "        bpc = loss_item / math.log(2)\n",
    "        ppl = math.exp(loss_item) if loss_item < 20 else float(\"inf\")\n",
    "\n",
    "        # Update tqdm line with key metrics\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{loss_item:.4f}\",\n",
    "            \"bpc\": f\"{bpc:.3f}\",\n",
    "            \"ppl\": f\"{ppl:.1f}\" if ppl != float(\"inf\") else \"inf\",\n",
    "            \"lr\": f\"{opt.lr:.6f}\",\n",
    "            \"gnorm\": f\"{grad_norm:.2f}\",\n",
    "        })\n",
    "\n",
    "        # TensorBoard logging\n",
    "        writer.add_scalar(\"loss/train_batch\", loss_item, global_step)\n",
    "        writer.add_scalar(\"lr\", opt.lr, global_step)\n",
    "        writer.add_scalar(\"grad_norm\", grad_norm, global_step)\n",
    "        global_step += 1\n",
    "\n",
    "    # End of epoch summary\n",
    "    tqdm.write(f\"[Epoch {epoch}] time={time.time() - epoch_start_time:.1f}s\")\n",
    "\n",
    "    # ---- Validation phase ----\n",
    "    val_loss = evaluate(val_loader)\n",
    "    bpc = val_loss / math.log(2)\n",
    "    tqdm.write(f\"[Epoch {epoch}] Val Loss {val_loss:.4f} | Val BPC {bpc:.4f}\")\n",
    "\n",
    "    writer.add_scalar(\"loss/val\", val_loss, epoch)\n",
    "    writer.add_scalar(\"metrics/val_bpc\", bpc, epoch)\n",
    "\n",
    "    # ---- Early stopping and checkpointing ----\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        bad_epochs = 0\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": base_opt.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_val\": best_val,\n",
    "            \"tok_chars\": tok.chars,\n",
    "            \"config\": dict(d_model=d_model, n_heads=n_heads, n_layers=n_layers, d_ff=d_ff,\n",
    "                           dropout=dropout, block_size=block_size, vocab_size=vocab_size)\n",
    "        }, save_path)\n",
    "        tqdm.write(f\"Improved! Best val_loss={best_val:.4f} (saved to {save_path})\")\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        tqdm.write(f\"No improvement ({bad_epochs}/{patience})\")\n",
    "        if bad_epochs >= patience:\n",
    "            tqdm.write(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe02fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Test best checkpoint + a sample generation\n",
    "# -----------------------------\n",
    "checkpoint = torch.load(save_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model with best_val={checkpoint['best_val']:.4f} at epoch={checkpoint['epoch']}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_simple(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        _, loss = model(xb, yb)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(len(loader), 1)\n",
    "\n",
    "test_loss = evaluate_simple(test_loader)\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Perplexity (PPL): {torch.exp(torch.tensor(test_loss)):.2f}\")\n",
    "print(f\"BPC: {(test_loss / math.log(2)):.4f}\")\n",
    "\n",
    "# Sample generation from a prompt\n",
    "prompt = \"ROMEO:\"\n",
    "start_ids = tok.encode(prompt)\n",
    "idx = torch.tensor([start_ids], dtype=torch.long, device=device)\n",
    "out = model.generate(idx, max_new_tokens=400, temperature=1.0, top_k=None)\n",
    "print(tok.decode(out[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
